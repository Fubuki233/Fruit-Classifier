{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing\n",
    "load and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"/path/to/train\"    # directory with subfolders for each class (train set)\n",
    "val_dir   = \"/path/to/validation\"  # directory with subfolders for each class (validation set)\n",
    "\n",
    "# Create ImageDataGenerators for loading images with normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)  # normalize pixel values\n",
    "val_datagen   = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Flow images in batches from directories\n",
    "batch_size = 32  # will be tuned via grid search\n",
    "IMG_SIZE = 224   # image size (can be adjusted as needed)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=True)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0):\n",
    "    model = models.Sequential()\n",
    "    # Convolutional layers with chosen activation and optional L2 regularization\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation=activation, \n",
    "                             input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    # Flatten and Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(32, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we parameterize the activation function, dropout rate, and L2 regularization rate so that these can be tuned. By default, the original model used ReLU activations and a dropout rate of 0.2 in two places. We include the option to use Leaky ReLU as an alternative; if Leaky ReLU is selected, we will insert layers.LeakyReLU() layers after each linear layer (since Keras layers.Dense or Conv2D do not accept leaky_relu string directly, we would use activation=None and add a LeakyReLU layer manually).\n",
    "\n",
    "Regularization: We have two forms of regularization to consider – dropout and L2 weight decay. Dropout randomly zeros out a fraction of neurons during training to prevent co-adaptation of features, while L2 penalizes large weights. Both are known to help reduce overfitting. In the model, dropout layers are included as shown; L2 regularization is applied to convolutional and dense layers via kernel_regularizer=regularizers.l2(l2_rate). We will tune the dropout_rate (e.g., try values like 0.0 = no dropout vs 0.5) and the l2_rate (e.g., 0.0 = no L2 vs a small value like 0.001) as hyperparameters.\n",
    "\n",
    "Initially, we keep other aspects constant (the number of layers/units as given). The model will be compiled with a chosen optimizer and learning rate (also to be tuned). For example, using Adam optimizer with a certain learning rate as in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)  # example learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with Grid Search and K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically find the best hyperparameter combination, we employ grid search over the specified hyperparameter ranges, coupled with K-fold cross-validation for robust evaluation. Grid search will exhaustively try all combinations of the provided hyperparameters, and K-fold CV means that for each combination, the training data is further split into K folds to evaluate the model’s performance across different subsets. This helps ensure the hyperparameter choice generalizes well and is not overfitting to one particular train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "\n",
    "Learning Rate: We will search values in the range 0.1 to 0.001. Given the prompt’s suggestion, we use a logarithmic scale: e.g. [0.1, 0.01, 0.001]. \n",
    "\n",
    "Optimizer: We consider two optimizers – Stochastic Gradient Descent (SGD) and Adam. These represent different update algorithms; SGD could be used with momentum, but here we’ll use plain SGD vs Adam.\n",
    "\n",
    "Batch Size: Try [16, 32, 64]. Batch size affects training stability and speed.\n",
    "\n",
    "Activation Function: Either ReLU or LeakyReLU. We will implement LeakyReLU with a negative slope (default 0.2) if chosen.\n",
    "\n",
    "Dropout Rate: Try e.g. [0.0, 0.5] – either no dropout or 50% dropout. (We could also test an intermediate like 0.2 as in the original.)\n",
    "\n",
    "L2 Regularization (weight decay) factor: Try [0.0, 0.001] – either no L2 penalty or a small penalty.\n",
    "\n",
    "Given these choices, the grid has 3×2×3×2×2×2 = 144 possible combinations.  For demonstration, let's set K = 5 (5-fold cross-validation) by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model(activation=activation, dropout_rate=dropout_rate, l2_rate=l2_rate)\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_clf = KerasClassifier(build_fn=build_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'activation': ['relu', 'leaky_relu'],\n",
    "    'dropout_rate': [0.0, 0.5],\n",
    "    'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "# Set up GridSearchCV with K-fold cross-validation\n",
    "K = 5  # (this can be changed as needed)\n",
    "grid_search = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=K, n_jobs=1, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how the grid search with cross-validation can be set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model(activation=activation, dropout_rate=dropout_rate, l2_rate=l2_rate)\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_clf = KerasClassifier(build_fn=build_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'activation': ['relu', 'leaky_relu'],\n",
    "    'dropout_rate': [0.0, 0.5],\n",
    "    'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "# Set up GridSearchCV with K-fold cross-validation\n",
    "K = 5  # (this can be changed as needed)\n",
    "grid_search = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=K, n_jobs=1, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above:\n",
    "\n",
    "We defined build_model to accept the hyperparams. If activation='leaky_relu', inside create_cnn_model we would handle that by setting layers with no activation and adding LeakyReLU layers. (This implementation detail can be handled with an if inside create_cnn_model.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_train, y_train are the training data and labels loaded in memory\n",
    "grid_result = grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_result.best_params_\n",
    "best_score = grid_result.best_score_\n",
    "print(\"Best cross-val accuracy: {:.4f}\".format(best_score))\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ideally, each fold’s training could stop early if the model’s performance on that fold’s validation subset stops improving. The scikit-learn wrapper does not directly use the K-fold partition as a Keras validation in each fit call. A workaround is to use validation_split within each fold’s training or to manually perform the cross-validation loop. In our case, we set a relatively small number of epochs (10) for each training, assuming this is sufficient to evaluate performance without severe overfitting. If we wanted to incorporate early stopping in GridSearchCV, we could pass a callback through the fit_params. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# grid_search.fit(X_train, y_train, **{'callbacks': [early_stop], 'validation_split': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would use 10% of each fold’s training data as a validation for early stopping. However,  this means we’re not fully using that 10% for training in each fold (since it’s used as a temp validation), and the actual cross-val fold (held-out by GridSearch) isn’t directly used for early stopping. Due to these complexities, one might simply keep epochs low or perform manual K-fold training to properly utilize each fold’s validation. Given our epoch count is modest (10) and we have early stopping for the final training phase, we can omit early stopping during the grid search phase to simplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with Best Hyperparameters and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model with best hyperparams\n",
    "best_model = create_cnn_model(activation=best_params['activation'], \n",
    "                              dropout_rate=best_params['dropout_rate'], \n",
    "                              l2_rate=best_params['l2_rate'])\n",
    "if best_params['optimizer_name'] == 'Adam':\n",
    "    final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "else:\n",
    "    final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_params['learning_rate'])\n",
    "best_model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup callbacks for early stopping (and optional learning rate reduction, checkpoints as in original code)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = best_model.fit(train_generator, epochs=50,  # start with an upper bound, early stopping will likely stop earlier\n",
    "                         validation_data=val_generator, \n",
    "                         callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Visualization of Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = best_model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.2%}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract history data\n",
    "epochs = range(1, len(history.history['loss'])+1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Plot Loss Curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "# Plot Accuracy Curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
