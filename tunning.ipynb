{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 22:42:57.683736: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 22:42:57.778543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 22:42:59.597741: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing\n",
    "load and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 311 images belonging to 4 classes.\n",
      "Found 60 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"/home/zyh/Fruit-Classifier/data/train_augment\"    # directory with subfolders for each class (train set)\n",
    "test_dir   = \"/home/zyh/Fruit-Classifier/data/test\"  # directory with subfolders for each class (validation set)\n",
    "\n",
    "# Create ImageDataGenerators for loading images with normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)  # normalize pixel values\n",
    "val_datagen   = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Flow images in batches from directories\n",
    "batch_size = 32  # will be tuned via grid search\n",
    "IMG_SIZE = 224   # image size (can be adjusted as needed)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=True)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    test_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0):\n",
    "    model = models.Sequential()\n",
    "    # Convolutional layers with chosen activation and optional L2 regularization\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation=activation, \n",
    "                             input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    # Flatten and Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(32, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we parameterize the activation function, dropout rate, and L2 regularization rate so that these can be tuned. By default, the original model used ReLU activations and a dropout rate of 0.2 in two places. We include the option to use Leaky ReLU as an alternative; if Leaky ReLU is selected, we will insert layers.LeakyReLU() layers after each linear layer (since Keras layers.Dense or Conv2D do not accept leaky_relu string directly, we would use activation=None and add a LeakyReLU layer manually).\n",
    "\n",
    "Regularization: We have two forms of regularization to consider ‚Äì dropout and L2 weight decay. Dropout randomly zeros out a fraction of neurons during training to prevent co-adaptation of features, while L2 penalizes large weights. Both are known to help reduce overfitting. In the model, dropout layers are included as shown; L2 regularization is applied to convolutional and dense layers via kernel_regularizer=regularizers.l2(l2_rate). We will tune the dropout_rate (e.g., try values like 0.0 = no dropout vs 0.5) and the l2_rate (e.g., 0.0 = no L2 vs a small value like 0.001) as hyperparameters.\n",
    "\n",
    "Initially, we keep other aspects constant (the number of layers/units as given). The model will be compiled with a chosen optimizer and learning rate (also to be tuned). For example, using Adam optimizer with a certain learning rate as in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765550581.979449  414352 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)  # example learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with Grid Search and K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically find the best hyperparameter combination, we employ grid search over the specified hyperparameter ranges, coupled with K-fold cross-validation for robust evaluation. Grid search will exhaustively try all combinations of the provided hyperparameters, and K-fold CV means that for each combination, the training data is further split into K folds to evaluate the model‚Äôs performance across different subsets. This helps ensure the hyperparameter choice generalizes well and is not overfitting to one particular train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "\n",
    "Learning Rate: We will search values in the range 0.1 to 0.001. Given the prompt‚Äôs suggestion, we use a logarithmic scale: e.g. [0.1, 0.01, 0.001]. \n",
    "\n",
    "Optimizer: We consider two optimizers ‚Äì Stochastic Gradient Descent (SGD) and Adam. These represent different update algorithms; SGD could be used with momentum, but here we‚Äôll use plain SGD vs Adam.\n",
    "\n",
    "Batch Size: Try [16, 32, 64]. Batch size affects training stability and speed.\n",
    "\n",
    "Activation Function: Either ReLU or LeakyReLU. We will implement LeakyReLU with a negative slope (default 0.2) if chosen.\n",
    "\n",
    "Dropout Rate: Try e.g. [0.0, 0.5] ‚Äì either no dropout or 50% dropout. (We could also test an intermediate like 0.2 as in the original.)\n",
    "\n",
    "L2 Regularization (weight decay) factor: Try [0.0, 0.001] ‚Äì either no L2 penalty or a small penalty.\n",
    "\n",
    "Given these choices, the grid has 3√ó2√ó3√ó2√ó2√ó2 = 144 possible combinations.  For demonstration, let's set K = 5 (5-fold cross-validation) by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model(activation=activation, dropout_rate=dropout_rate, l2_rate=l2_rate)\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ÂÖ®ÂÜÖÂ≠òÊï∞ÊçÆÂä†ËΩΩÂáΩÊï∞ (ÈÄüÂ∫¶‰ºòÂåñÁöÑÂÖ≥ÈîÆ)\n",
    "def load_data_to_memory(data_dir, img_size):\n",
    "    print(f\"Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆÂà∞ÂÜÖÂ≠ò: {data_dir} ...\")\n",
    "    X = []\n",
    "    y = []\n",
    "    # Á°Æ‰øùÁ±ªÂêçÈ°∫Â∫è‰∏ÄËá¥: ['apple', 'banana', 'mixed', 'orange']\n",
    "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    print(f\"Ê£ÄÊµãÂà∞ÁöÑÁ±ªÂà´: {classes}\")\n",
    "    \n",
    "    for label_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        files = os.listdir(class_dir)\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                fpath = os.path.join(class_dir, fname)\n",
    "                # Âä†ËΩΩÂπ∂Ë∞ÉÊï¥Â§ßÂ∞è\n",
    "                img = load_img(fpath, target_size=(img_size, img_size))\n",
    "                # ËΩ¨‰∏∫Êï∞ÁªÑÂπ∂ÂΩí‰∏ÄÂåñ (0-1)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                \n",
    "                X.append(img_array)\n",
    "                y.append(label_idx)\n",
    "                \n",
    "    X = np.array(X)\n",
    "    # Â∞ÜÊ†áÁ≠æËΩ¨‰∏∫ One-hot ÁºñÁ†Å (e.g., [0, 1, 0, 0])\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=len(classes))\n",
    "    print(f\"Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê! X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# ‰∏ÄÊ¨°ÊÄßËØªÂèñÊâÄÊúâÊï∞ÊçÆÂà∞ RAM (320Âº†ÂõæÁ∫¶Âç†Áî® 150MB~200MB ÂÜÖÂ≠òÔºåÈùûÂ∏∏ÂÆâÂÖ®)\n",
    "X_all, y_all = load_data_to_memory(train_dir, IMG_SIZE)\n",
    "\n",
    "\n",
    "#  ÊûÅÈÄüÁâà Grid Search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001,0.005],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32],\n",
    "    'activation': ['relu', 'leaky_relu'],\n",
    "    'dropout_rate': [0.0, 0.5],\n",
    "    #'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "\n",
    "# ËøôÈáåÁöÑ shuffle=True ÂæàÈáçË¶Å\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "# ÁîüÊàêÊâÄÊúâÁªÑÂêà\n",
    "combinations = list(itertools.product(*param_grid.values()))\n",
    "total_runs = len(combinations) * 5\n",
    "current_run = 0\n",
    "\n",
    "print(f\"ÂºÄÂßã Grid SearchÔºåÊÄªÂÖ±Ë¶ÅËÆ≠ÁªÉ {len(combinations)} ‰∏™Ê®°ÂûãÈÖçÁΩÆ...\")\n",
    "\n",
    "for combo in combinations:\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    \n",
    "    # ‰ªé params ‰∏≠ÂàÜÁ¶ªÂá∫ batch_sizeÔºåÂõ†‰∏∫ÂÆÉ‰∏ç‰º†Áªô build_model\n",
    "    build_args = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "    current_batch_size = params['batch_size']\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    # K-Fold Âæ™ÁéØ\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "        # Ê∏ÖÁêÜÊóßÊ®°ÂûãÔºåÈáäÊîæÊòæÂ≠ò (ÂÖ≥ÈîÆÊ≠•È™§ÔºÅ)\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # ÂàíÂàÜÊï∞ÊçÆ (Áõ¥Êé•ÂÜÖÂ≠òÂàáÁâáÔºåÈÄüÂ∫¶ÊûÅÂø´)\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "        \n",
    "        # ÊûÑÂª∫Ê®°Âûã\n",
    "        model = build_model(**build_args)\n",
    "        \n",
    "        # ÂõûË∞ÉÂáΩÊï∞\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss'),\n",
    "            # ÁΩëÊ†ºÊêúÁ¥¢Êó∂ ReduceLROnPlateau ÂèØËÉΩÊãñÊÖ¢ÈÄüÂ∫¶ÔºåÁÆÄÂçïËµ∑ËßÅÂèØ‰ª•ÂÖàÊ≥®ÈáäÊéâÔºåÊàñËÄÖ‰øùÁïô\n",
    "            # tf.keras.callbacks.ReduceLROnPlateau(patience=2) \n",
    "        ]\n",
    "        \n",
    "        # ËÆ≠ÁªÉ (Êó†ÈúÄ GeneratorÔºåÁõ¥Êé•‰º†Êï∞ÁªÑ)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=current_batch_size,\n",
    "            epochs=10, \n",
    "            callbacks=callbacks,\n",
    "            verbose=0  # ÈùôÈªòÊ®°ÂºèÔºåÂè™ÊâìÂç∞ÁªìÊûú\n",
    "        )\n",
    "        \n",
    "        # ËØÑ‰º∞\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        acc_list.append(val_acc)\n",
    "        \n",
    "        current_run += 1\n",
    "        # ÊâìÂç∞ËøõÂ∫¶Êù°\n",
    "        print(f\"Run {current_run}/{total_runs} - Acc: {val_acc:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    print(f\"Params: {params} | Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        best_params = params\n",
    "        print(f\"üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"ÊêúÁ¥¢ÁªìÊùü\")\n",
    "print(f\"ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc}\")\n",
    "print(f\"ÊúÄ‰Ω≥ÂèÇÊï∞: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above:\n",
    "\n",
    "We defined build_model to accept the hyperparams. If activation='leaky_relu', inside create_cnn_model we would handle that by setting layers with no activation and adding LeakyReLU layers. (This implementation detail can be handled with an if inside create_cnn_model.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ideally, each fold‚Äôs training could stop early if the model‚Äôs performance on that fold‚Äôs validation subset stops improving. The scikit-learn wrapper does not directly use the K-fold partition as a Keras validation in each fit call. A workaround is to use validation_split within each fold‚Äôs training or to manually perform the cross-validation loop. In our case, we set a relatively small number of epochs (10) for each training, assuming this is sufficient to evaluate performance without severe overfitting. If we wanted to incorporate early stopping in GridSearchCV, we could pass a callback through the fit_params. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# grid_search.fit(X_train, y_train, **{'callbacks': [early_stop], 'validation_split': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would use 10% of each fold‚Äôs training data as a validation for early stopping. However,  this means we‚Äôre not fully using that 10% for training in each fold (since it‚Äôs used as a temp validation), and the actual cross-val fold (held-out by GridSearch) isn‚Äôt directly used for early stopping. Due to these complexities, one might simply keep epochs low or perform manual K-fold training to properly utilize each fold‚Äôs validation. Given our epoch count is modest (10) and we have early stopping for the final training phase, we can omit early stopping during the grid search phase to simplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with Best Hyperparameters and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model with best hyperparams\n",
    "best_model = create_cnn_model(activation=best_params['activation'], \n",
    "                              dropout_rate=best_params['dropout_rate'], \n",
    "                              l2_rate=best_params['l2_rate'])\n",
    "if best_params['optimizer_name'] == 'Adam':\n",
    "    final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "else:\n",
    "    final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_params['learning_rate'])\n",
    "best_model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup callbacks for early stopping (and optional learning rate reduction, checkpoints as in original code)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = best_model.fit(train_generator, epochs=50,  # start with an upper bound, early stopping will likely stop earlier\n",
    "                         validation_data=val_generator, \n",
    "                         callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Visualization of Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = best_model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.2%}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract history data\n",
    "epochs = range(1, len(history.history['loss'])+1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Plot Loss Curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "# Plot Accuracy Curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
