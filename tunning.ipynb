{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\" # å…³é—­éƒ¨åˆ† XLA è¡Œä¸º\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\" # å¼ºåˆ¶ç¡®å®šæ€§ç®—æ³•ï¼Œå‡å°‘æœç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 17:43:55.136104: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-14 17:43:55.214563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-14 17:43:56.939795: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing\n",
    "load and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"/home/zyh/Fruit-Classifier/data/train_augment\"    # directory with subfolders for each class (train set)\n",
    "\n",
    "\n",
    "IMG_SIZE = 224   # image size (can be adjusted as needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0):\n",
    "    model = models.Sequential()\n",
    "    # Convolutional layers with chosen activation and optional L2 regularization\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation=activation, \n",
    "                             input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    # Flatten and Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(32, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we parameterize the activation function, dropout rate, and L2 regularization rate so that these can be tuned. By default, the original model used ReLU activations and a dropout rate of 0.2 in two places. We include the option to use Leaky ReLU as an alternative; if Leaky ReLU is selected, we will insert layers.LeakyReLU() layers after each linear layer (since Keras layers.Dense or Conv2D do not accept leaky_relu string directly, we would use activation=None and add a LeakyReLU layer manually).\n",
    "\n",
    "Regularization: We have two forms of regularization to consider â€“ dropout and L2 weight decay. Dropout randomly zeros out a fraction of neurons during training to prevent co-adaptation of features, while L2 penalizes large weights. Both are known to help reduce overfitting. In the model, dropout layers are included as shown; L2 regularization is applied to convolutional and dense layers via kernel_regularizer=regularizers.l2(l2_rate). We will tune the dropout_rate (e.g., try values like 0.0 = no dropout vs 0.5) and the l2_rate (e.g., 0.0 = no L2 vs a small value like 0.001) as hyperparameters.\n",
    "\n",
    "Initially, we keep other aspects constant (the number of layers/units as given). The model will be compiled with a chosen optimizer and learning rate (also to be tuned). For example, using Adam optimizer with a certain learning rate as in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)  # example learning rate\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with Grid Search and K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically find the best hyperparameter combination, we employ grid search over the specified hyperparameter ranges, coupled with K-fold cross-validation for robust evaluation. Grid search will exhaustively try all combinations of the provided hyperparameters, and K-fold CV means that for each combination, the training data is further split into K folds to evaluate the modelâ€™s performance across different subsets. This helps ensure the hyperparameter choice generalizes well and is not overfitting to one particular train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "\n",
    "Learning Rate: We will search values in the range 0.1 to 0.001. Given the promptâ€™s suggestion, we use a logarithmic scale: e.g. [0.1, 0.01, 0.001]. \n",
    "\n",
    "Optimizer: We consider two optimizers â€“ Stochastic Gradient Descent (SGD) and Adam. These represent different update algorithms; SGD could be used with momentum, but here weâ€™ll use plain SGD vs Adam.\n",
    "\n",
    "Batch Size: Try [16, 32, 64]. Batch size affects training stability and speed.\n",
    "\n",
    "Activation Function: Either ReLU or LeakyReLU. We will implement LeakyReLU with a negative slope (default 0.2) if chosen.\n",
    "\n",
    "Dropout Rate: Try e.g. [0.0, 0.5] â€“ either no dropout or 50% dropout. (We could also test an intermediate like 0.2 as in the original.)\n",
    "\n",
    "L2 Regularization (weight decay) factor: Try [0.0, 0.001] â€“ either no L2 penalty or a small penalty.\n",
    "\n",
    "Given these choices, the grid has 3Ã—2Ã—3Ã—2Ã—2Ã—2 = 144 possible combinations.  For demonstration, let's set K = 5 (5-fold cross-validation) by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ•°æ®åˆ°å†…å­˜: /home/zyh/Fruit-Classifier/data/train_augment ...\n",
      "æ£€æµ‹åˆ°çš„ç±»åˆ«: ['apple', 'banana', 'mixed', 'orange']\n",
      "æ•°æ®åŠ è½½å®Œæˆ! X shape: (309, 224, 224, 3), y shape: (309, 4)\n",
      "å¼€å§‹ Grid Searchï¼Œæ€»å…±è¦è®­ç»ƒ 24 ä¸ªæ¨¡åž‹é…ç½®...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765705440.567134  456215 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-12-14 17:44:02.087842: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/120 - Acc: 0.4677\n",
      "Run 2/120 - Acc: 0.4677\n",
      "Run 3/120 - Acc: 0.5484\n",
      "Run 4/120 - Acc: 0.7097\n",
      "Run 5/120 - Acc: 0.4262\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.5240\n",
      "ðŸ”¥ðŸ”¥ðŸ”¥ å‘çŽ°æ–°æœ€ä½³ç²¾åº¦: 0.5240\n",
      "Run 6/120 - Acc: 0.3871\n",
      "Run 7/120 - Acc: 0.2419\n",
      "Run 8/120 - Acc: 0.2419\n",
      "Run 9/120 - Acc: 0.4194\n",
      "Run 10/120 - Acc: 0.5410\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3663\n",
      "Run 11/120 - Acc: 0.5806\n",
      "Run 12/120 - Acc: 0.6613\n",
      "Run 13/120 - Acc: 0.5323\n",
      "Run 14/120 - Acc: 0.6452\n",
      "Run 15/120 - Acc: 0.5902\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.6019\n",
      "ðŸ”¥ðŸ”¥ðŸ”¥ å‘çŽ°æ–°æœ€ä½³ç²¾åº¦: 0.6019\n",
      "Run 16/120 - Acc: 0.3871\n",
      "Run 17/120 - Acc: 0.1290\n",
      "Run 18/120 - Acc: 0.5484\n",
      "Run 19/120 - Acc: 0.2258\n",
      "Run 20/120 - Acc: 0.3443\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3269\n",
      "Run 21/120 - Acc: 0.2258\n",
      "Run 22/120 - Acc: 0.2258\n",
      "Run 23/120 - Acc: 0.1613\n",
      "Run 24/120 - Acc: 0.2258\n",
      "Run 25/120 - Acc: 0.6885\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.3054\n",
      "Run 26/120 - Acc: 0.2419\n",
      "Run 27/120 - Acc: 0.2258\n",
      "Run 28/120 - Acc: 0.1613\n",
      "Run 29/120 - Acc: 0.2258\n",
      "Run 30/120 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2169\n",
      "Run 31/120 - Acc: 0.2903\n",
      "Run 32/120 - Acc: 0.2258\n",
      "Run 33/120 - Acc: 0.2581\n",
      "Run 34/120 - Acc: 0.2581\n",
      "Run 35/120 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.2524\n",
      "Run 36/120 - Acc: 0.2419\n",
      "Run 37/120 - Acc: 0.2258\n",
      "Run 38/120 - Acc: 0.1613\n",
      "Run 39/120 - Acc: 0.2258\n",
      "Run 40/120 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2169\n",
      "Run 41/120 - Acc: 0.4194\n",
      "Run 42/120 - Acc: 0.1452\n",
      "Run 43/120 - Acc: 0.1935\n",
      "Run 44/120 - Acc: 0.3871\n",
      "Run 45/120 - Acc: 0.4262\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.3143\n",
      "Run 46/120 - Acc: 0.4516\n",
      "Run 47/120 - Acc: 0.2419\n",
      "Run 48/120 - Acc: 0.3387\n",
      "Run 49/120 - Acc: 0.4677\n",
      "Run 50/120 - Acc: 0.4262\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3852\n",
      "Run 51/120 - Acc: 0.4194\n",
      "Run 52/120 - Acc: 0.4516\n",
      "Run 53/120 - Acc: 0.4194\n",
      "Run 54/120 - Acc: 0.4839\n",
      "Run 55/120 - Acc: 0.4262\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.4401\n",
      "Run 56/120 - Acc: 0.3226\n",
      "Run 57/120 - Acc: 0.2742\n",
      "Run 58/120 - Acc: 0.2742\n",
      "Run 59/120 - Acc: 0.3065\n",
      "Run 60/120 - Acc: 0.3443\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3043\n",
      "Run 61/120 - Acc: 0.8710\n",
      "Run 62/120 - Acc: 0.8065\n",
      "Run 63/120 - Acc: 0.8710\n",
      "Run 64/120 - Acc: 0.7419\n",
      "Run 65/120 - Acc: 0.7869\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.8154\n",
      "ðŸ”¥ðŸ”¥ðŸ”¥ å‘çŽ°æ–°æœ€ä½³ç²¾åº¦: 0.8154\n",
      "Run 66/120 - Acc: 0.8387\n",
      "Run 67/120 - Acc: 0.8226\n",
      "Run 68/120 - Acc: 0.7581\n",
      "Run 69/120 - Acc: 0.6129\n",
      "Run 70/120 - Acc: 0.7541\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.7573\n",
      "Run 71/120 - Acc: 0.7903\n",
      "Run 72/120 - Acc: 0.8548\n",
      "Run 73/120 - Acc: 0.7419\n",
      "Run 74/120 - Acc: 0.6935\n",
      "Run 75/120 - Acc: 0.8197\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.7801\n",
      "Run 76/120 - Acc: 0.8871\n",
      "Run 77/120 - Acc: 0.8387\n",
      "Run 78/120 - Acc: 0.7581\n",
      "Run 79/120 - Acc: 0.8065\n",
      "Run 80/120 - Acc: 0.7869\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.8154\n",
      "Run 81/120 - Acc: 0.4032\n",
      "Run 82/120 - Acc: 0.6452\n",
      "Run 83/120 - Acc: 0.6452\n",
      "Run 84/120 - Acc: 0.5323\n",
      "Run 85/120 - Acc: 0.6066\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.5665\n",
      "Run 86/120 - Acc: 0.2419\n",
      "Run 87/120 - Acc: 0.3226\n",
      "Run 88/120 - Acc: 0.1774\n",
      "Run 89/120 - Acc: 0.2742\n",
      "Run 90/120 - Acc: 0.6066\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3245\n",
      "Run 91/120 - Acc: 0.5000\n",
      "Run 92/120 - Acc: 0.2742\n",
      "Run 93/120 - Acc: 0.5161\n",
      "Run 94/120 - Acc: 0.5161\n",
      "Run 95/120 - Acc: 0.4098\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.4433\n",
      "Run 96/120 - Acc: 0.5484\n",
      "Run 97/120 - Acc: 0.4839\n",
      "Run 98/120 - Acc: 0.2742\n",
      "Run 99/120 - Acc: 0.3226\n",
      "Run 100/120 - Acc: 0.2459\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3750\n",
      "Run 101/120 - Acc: 0.8065\n",
      "Run 102/120 - Acc: 0.1290\n",
      "Run 103/120 - Acc: 0.1613\n",
      "Run 104/120 - Acc: 0.2097\n",
      "Run 105/120 - Acc: 0.4918\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.3597\n",
      "Run 106/120 - Acc: 0.2258\n",
      "Run 107/120 - Acc: 0.2258\n",
      "Run 108/120 - Acc: 0.1613\n",
      "Run 109/120 - Acc: 0.2258\n",
      "Run 110/120 - Acc: 0.2623\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2202\n",
      "Run 111/120 - Acc: 0.2419\n",
      "Run 112/120 - Acc: 0.7258\n",
      "Run 113/120 - Acc: 0.7581\n",
      "Run 114/120 - Acc: 0.5968\n",
      "Run 115/120 - Acc: 0.8197\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.6285\n",
      "Run 116/120 - Acc: 0.2903\n",
      "Run 117/120 - Acc: 0.2258\n",
      "Run 118/120 - Acc: 0.1613\n",
      "Run 119/120 - Acc: 0.4677\n",
      "Run 120/120 - Acc: 0.2623\n",
      "Params: {'learning_rate': 0.005, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2815\n",
      "\n",
      "========================================\n",
      "æœç´¢ç»“æŸ\n",
      "æœ€ä½³ç²¾åº¦: 0.8154415726661682\n",
      "æœ€ä½³å‚æ•°: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model(activation=activation, dropout_rate=dropout_rate, l2_rate=l2_rate)\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], jit_compile=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "# å…¨å†…å­˜æ•°æ®åŠ è½½å‡½æ•° (é€Ÿåº¦ä¼˜åŒ–çš„å…³é”®)\n",
    "def load_data_to_memory(data_dir, img_size):\n",
    "    print(f\"æ­£åœ¨åŠ è½½æ•°æ®åˆ°å†…å­˜: {data_dir} ...\")\n",
    "    X = []\n",
    "    y = []\n",
    "    # ç¡®ä¿ç±»åé¡ºåºä¸€è‡´: ['apple', 'banana', 'mixed', 'orange']\n",
    "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    print(f\"æ£€æµ‹åˆ°çš„ç±»åˆ«: {classes}\")\n",
    "    \n",
    "    for label_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        files = os.listdir(class_dir)\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                fpath = os.path.join(class_dir, fname)\n",
    "                # åŠ è½½å¹¶è°ƒæ•´å¤§å°\n",
    "                img = load_img(fpath, target_size=(img_size, img_size))\n",
    "                # è½¬ä¸ºæ•°ç»„å¹¶å½’ä¸€åŒ– (0-1)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                \n",
    "                X.append(img_array)\n",
    "                y.append(label_idx)\n",
    "                \n",
    "    X = np.array(X)\n",
    "    # å°†æ ‡ç­¾è½¬ä¸º One-hot ç¼–ç  (e.g., [0, 1, 0, 0])\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=len(classes))\n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆ! X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®åˆ° RAM (320å¼ å›¾çº¦å ç”¨ 150MB~200MB å†…å­˜ï¼Œéžå¸¸å®‰å…¨)\n",
    "X_all, y_all = load_data_to_memory(train_dir, IMG_SIZE)\n",
    "\n",
    "\n",
    "#  æžé€Ÿç‰ˆ Grid Search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001,0.005],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32],\n",
    "    'activation': ['relu'],\n",
    "    'dropout_rate': [0.0, 0.5],\n",
    "    #'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "# ç”Ÿæˆæ‰€æœ‰ç»„åˆ\n",
    "combinations = list(itertools.product(*param_grid.values()))\n",
    "total_runs = len(combinations) * 5\n",
    "current_run = 0\n",
    "\n",
    "print(f\"å¼€å§‹ Grid Searchï¼Œæ€»å…±è¦è®­ç»ƒ {len(combinations)} ä¸ªæ¨¡åž‹é…ç½®...\")\n",
    "\n",
    "for combo in combinations:\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    \n",
    "    # ä»Ž params ä¸­åˆ†ç¦»å‡º batch_sizeï¼Œå› ä¸ºå®ƒä¸ä¼ ç»™ build_model\n",
    "    build_args = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "    current_batch_size = params['batch_size']\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    # K-Fold å¾ªçŽ¯\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "        # æ¸…ç†æ—§æ¨¡åž‹ï¼Œé‡Šæ”¾æ˜¾å­˜ \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # åˆ’åˆ†æ•°æ® (ç›´æŽ¥å†…å­˜åˆ‡ç‰‡ï¼Œé€Ÿåº¦æžå¿«)\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "        \n",
    "        # æž„å»ºæ¨¡åž‹\n",
    "        model = build_model(**build_args)\n",
    "        \n",
    "        # å›žè°ƒå‡½æ•°\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=False, monitor='val_loss'),    #å…ˆä¸ä¿ç•™æƒé‡\n",
    "            # ç½‘æ ¼æœç´¢æ—¶ ReduceLROnPlateau å¯èƒ½æ‹–æ…¢é€Ÿåº¦ï¼Œç®€å•èµ·è§å¯ä»¥å…ˆæ³¨é‡ŠæŽ‰ï¼Œæˆ–è€…ä¿ç•™\n",
    "            # tf.keras.callbacks.ReduceLROnPlateau(patience=2) \n",
    "        ]\n",
    "        \n",
    "        # è®­ç»ƒ (æ— éœ€ Generatorï¼Œç›´æŽ¥ä¼ æ•°ç»„)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=current_batch_size,\n",
    "            epochs=15, \n",
    "            callbacks=callbacks,\n",
    "            verbose=0  # é™é»˜æ¨¡å¼ï¼Œåªæ‰“å°ç»“æžœ\n",
    "        )\n",
    "        \n",
    "        # è¯„ä¼°\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        acc_list.append(val_acc)\n",
    "        \n",
    "        current_run += 1\n",
    "        # æ‰“å°è¿›åº¦æ¡\n",
    "        print(f\"Run {current_run}/{total_runs} - Acc: {val_acc:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    print(f\"Params: {params} | Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        best_params = params\n",
    "        print(f\"ðŸ”¥ðŸ”¥ðŸ”¥ å‘çŽ°æ–°æœ€ä½³ç²¾åº¦: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"æœç´¢ç»“æŸ\")\n",
    "print(f\"æœ€ä½³ç²¾åº¦: {best_acc}\")\n",
    "print(f\"æœ€ä½³å‚æ•°: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above:\n",
    "\n",
    "We defined build_model to accept the hyperparams. If activation='leaky_relu', inside create_cnn_model we would handle that by setting layers with no activation and adding LeakyReLU layers. (This implementation detail can be handled with an if inside create_cnn_model.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ideally, each foldâ€™s training could stop early if the modelâ€™s performance on that foldâ€™s validation subset stops improving. The scikit-learn wrapper does not directly use the K-fold partition as a Keras validation in each fit call. A workaround is to use validation_split within each foldâ€™s training or to manually perform the cross-validation loop. In our case, we set a relatively small number of epochs (10) for each training, assuming this is sufficient to evaluate performance without severe overfitting. If we wanted to incorporate early stopping in GridSearchCV, we could pass a callback through the fit_params. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# grid_search.fit(X_train, y_train, **{'callbacks': [early_stop], 'validation_split': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would use 10% of each foldâ€™s training data as a validation for early stopping. However,  this means weâ€™re not fully using that 10% for training in each fold (since itâ€™s used as a temp validation), and the actual cross-val fold (held-out by GridSearch) isnâ€™t directly used for early stopping. Due to these complexities, one might simply keep epochs low or perform manual K-fold training to properly utilize each foldâ€™s validation. Given our epoch count is modest (10) and we have early stopping for the final training phase, we can omit early stopping during the grid search phase to simplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with Best Hyperparameters and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'l2_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build final model with best hyperparams\u001b[39;00m\n\u001b[1;32m      2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m create_cnn_model(activation\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      3\u001b[0m                               dropout_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m----> 4\u001b[0m                               l2_rate\u001b[38;5;241m=\u001b[39m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml2_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m     final_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'l2_rate'"
     ]
    }
   ],
   "source": [
    "# # Build final model with best hyperparams\n",
    "# best_model = create_cnn_model(activation=best_params['activation'], \n",
    "#                               dropout_rate=best_params['dropout_rate'], \n",
    "#                               l2_rate=best_params['l2_rate'])\n",
    "# if best_params['optimizer_name'] == 'Adam':\n",
    "#     final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "# else:\n",
    "#     final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_params['learning_rate'])\n",
    "# best_model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Setup callbacks for early stopping (and optional learning rate reduction, checkpoints as in original code)\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "#     ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model with early stopping\n",
    "# history = best_model.fit(train_generator, epochs=50,  # start with an upper bound, early stopping will likely stop earlier\n",
    "#                          validation_data=val_generator, \n",
    "#                          callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Visualization of Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_acc = best_model.evaluate(val_generator)\n",
    "# print(f\"Validation Accuracy: {val_acc:.2%}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract history data\n",
    "# epochs = range(1, len(history.history['loss'])+1)\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# train_acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# # Plot Loss Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "# plt.title('Training vs Validation Loss')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "# # Plot Accuracy Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "# plt.title('Training vs Validation Accuracy')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
