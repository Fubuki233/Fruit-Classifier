{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\" # ÂÖ≥Èó≠ÈÉ®ÂàÜ XLA Ë°å‰∏∫\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\" # Âº∫Âà∂Á°ÆÂÆöÊÄßÁÆóÊ≥ïÔºåÂáèÂ∞ëÊêúÁ¥¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 23:18:56.080068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-14 23:18:56.113857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-14 23:18:57.169058: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Â∑≤ÂºÄÂêØÊòæÂ≠òÊåâÈúÄÂàÜÈÖçÊ®°Âºè\n"
     ]
    }
   ],
   "source": [
    "# 1. Á°Æ‰øùÊòæÂ≠òÊåâÈúÄÂàÜÈÖçÔºàÈò≤Ê≠¢ OOMÔºâ\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ Â∑≤ÂºÄÂêØÊòæÂ≠òÊåâÈúÄÂàÜÈÖçÊ®°Âºè\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing\n",
    "load and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"/home/zyh/Fruit-Classifier/data/train_augment\"    # directory with subfolders for each class (train set)\n",
    "\n",
    "\n",
    "IMG_SIZE = 224   # image size (can be adjusted as needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0):\n",
    "    model = models.Sequential()\n",
    "    # Convolutional layers with chosen activation and optional L2 regularization\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation=activation, \n",
    "                             input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same', activation=activation,\n",
    "                             kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    # Flatten and Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(32, activation=activation, \n",
    "                            kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(4, activation='softmax'))  # 4 output classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we parameterize the activation function, dropout rate, and L2 regularization rate so that these can be tuned. By default, the original model used ReLU activations and a dropout rate of 0.2 in two places. We include the option to use Leaky ReLU as an alternative; if Leaky ReLU is selected, we will insert layers.LeakyReLU() layers after each linear layer (since Keras layers.Dense or Conv2D do not accept leaky_relu string directly, we would use activation=None and add a LeakyReLU layer manually).\n",
    "\n",
    "Regularization: We have two forms of regularization to consider ‚Äì dropout and L2 weight decay. Dropout randomly zeros out a fraction of neurons during training to prevent co-adaptation of features, while L2 penalizes large weights. Both are known to help reduce overfitting. In the model, dropout layers are included as shown; L2 regularization is applied to convolutional and dense layers via kernel_regularizer=regularizers.l2(l2_rate). We will tune the dropout_rate (e.g., try values like 0.0 = no dropout vs 0.5) and the l2_rate (e.g., 0.0 = no L2 vs a small value like 0.001) as hyperparameters.\n",
    "\n",
    "Initially, we keep other aspects constant (the number of layers/units as given). The model will be compiled with a chosen optimizer and learning rate (also to be tuned). For example, using Adam optimizer with a certain learning rate as in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_cnn_model(activation='relu', dropout_rate=0.2, l2_rate=0.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)  # example learning rate\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with Grid Search and K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically find the best hyperparameter combination, we employ grid search over the specified hyperparameter ranges, coupled with K-fold cross-validation for robust evaluation. Grid search will exhaustively try all combinations of the provided hyperparameters, and K-fold CV means that for each combination, the training data is further split into K folds to evaluate the model‚Äôs performance across different subsets. This helps ensure the hyperparameter choice generalizes well and is not overfitting to one particular train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "\n",
    "Learning Rate: We will search values in the range 0.1 to 0.001. Given the prompt‚Äôs suggestion, we use a logarithmic scale: e.g. [0.1, 0.01, 0.001]. \n",
    "\n",
    "Optimizer: We consider two optimizers ‚Äì Stochastic Gradient Descent (SGD) and Adam. These represent different update algorithms; SGD could be used with momentum, but here we‚Äôll use plain SGD vs Adam.\n",
    "\n",
    "Batch Size: Try [16, 32, 64]. Batch size affects training stability and speed.\n",
    "\n",
    "Activation Function: Either ReLU or LeakyReLU. We will implement LeakyReLU with a negative slope (default 0.2) if chosen.\n",
    "\n",
    "Dropout Rate: Try e.g. [0.0, 0.5] ‚Äì either no dropout or 50% dropout. (We could also test an intermediate like 0.2 as in the original.)\n",
    "\n",
    "L2 Regularization (weight decay) factor: Try [0.0, 0.001] ‚Äì either no L2 penalty or a small penalty.\n",
    "\n",
    "Given these choices, the grid has 3√ó2√ó3√ó2√ó2√ó2 = 144 possible combinations.  For demonstration, let's set K = 5 (5-fold cross-validation) by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆÂà∞ÂÜÖÂ≠ò: /home/zyh/Fruit-Classifier/data/train_augment ...\n",
      "Ê£ÄÊµãÂà∞ÁöÑÁ±ªÂà´: ['apple', 'banana', 'mixed', 'orange']\n",
      "Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê! X shape: (308, 224, 224, 3), y shape: (308, 4)\n",
      "ÂºÄÂßã Grid SearchÔºåÊÄªÂÖ±Ë¶ÅËÆ≠ÁªÉ 36 ‰∏™Ê®°ÂûãÈÖçÁΩÆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765725542.398226  354978 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-12-14 23:19:03.957174: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/180 - Acc: 0.5645\n",
      "Run 2/180 - Acc: 0.4032\n",
      "Run 3/180 - Acc: 0.2258\n",
      "Run 4/180 - Acc: 0.6230\n",
      "Run 5/180 - Acc: 0.6066\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.4846\n",
      "üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: 0.4846\n",
      "Run 6/180 - Acc: 0.6290\n",
      "Run 7/180 - Acc: 0.3871\n",
      "Run 8/180 - Acc: 0.5484\n",
      "Run 9/180 - Acc: 0.4590\n",
      "Run 10/180 - Acc: 0.6721\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.5391\n",
      "üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: 0.5391\n",
      "Run 11/180 - Acc: 0.2742\n",
      "Run 12/180 - Acc: 0.7742\n",
      "Run 13/180 - Acc: 0.2903\n",
      "Run 14/180 - Acc: 0.2459\n",
      "Run 15/180 - Acc: 0.5738\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.4317\n",
      "Run 16/180 - Acc: 0.4355\n",
      "Run 17/180 - Acc: 0.4516\n",
      "Run 18/180 - Acc: 0.4032\n",
      "Run 19/180 - Acc: 0.5410\n",
      "Run 20/180 - Acc: 0.4426\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.4548\n",
      "Run 21/180 - Acc: 0.2419\n",
      "Run 22/180 - Acc: 0.5968\n",
      "Run 23/180 - Acc: 0.2419\n",
      "Run 24/180 - Acc: 0.7049\n",
      "Run 25/180 - Acc: 0.6230\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.4817\n",
      "Run 26/180 - Acc: 0.2419\n",
      "Run 27/180 - Acc: 0.1129\n",
      "Run 28/180 - Acc: 0.1774\n",
      "Run 29/180 - Acc: 0.3934\n",
      "Run 30/180 - Acc: 0.2459\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2343\n",
      "Run 31/180 - Acc: 0.2258\n",
      "Run 32/180 - Acc: 0.2258\n",
      "Run 33/180 - Acc: 0.1613\n",
      "Run 34/180 - Acc: 0.2623\n",
      "Run 35/180 - Acc: 0.2459\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.2242\n",
      "Run 36/180 - Acc: 0.2419\n",
      "Run 37/180 - Acc: 0.2258\n",
      "Run 38/180 - Acc: 0.1613\n",
      "Run 39/180 - Acc: 0.2295\n",
      "Run 40/180 - Acc: 0.2623\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.2242\n",
      "Run 41/180 - Acc: 0.2258\n",
      "Run 42/180 - Acc: 0.2258\n",
      "Run 43/180 - Acc: 0.1613\n",
      "Run 44/180 - Acc: 0.2623\n",
      "Run 45/180 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2209\n",
      "Run 46/180 - Acc: 0.5645\n",
      "Run 47/180 - Acc: 0.6935\n",
      "Run 48/180 - Acc: 0.1613\n",
      "Run 49/180 - Acc: 0.2295\n",
      "Run 50/180 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.3757\n",
      "Run 51/180 - Acc: 0.2419\n",
      "Run 52/180 - Acc: 0.2258\n",
      "Run 53/180 - Acc: 0.2903\n",
      "Run 54/180 - Acc: 0.2295\n",
      "Run 55/180 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.2434\n",
      "Run 56/180 - Acc: 0.2903\n",
      "Run 57/180 - Acc: 0.2258\n",
      "Run 58/180 - Acc: 0.1613\n",
      "Run 59/180 - Acc: 0.2295\n",
      "Run 60/180 - Acc: 0.2295\n",
      "Params: {'learning_rate': 0.01, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2273\n",
      "Run 61/180 - Acc: 0.5645\n",
      "Run 62/180 - Acc: 0.6129\n",
      "Run 63/180 - Acc: 0.5000\n",
      "Run 64/180 - Acc: 0.5574\n",
      "Run 65/180 - Acc: 0.3279\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.5125\n",
      "Run 66/180 - Acc: 0.2581\n",
      "Run 67/180 - Acc: 0.6774\n",
      "Run 68/180 - Acc: 0.3226\n",
      "Run 69/180 - Acc: 0.3443\n",
      "Run 70/180 - Acc: 0.4918\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.4188\n",
      "Run 71/180 - Acc: 0.3710\n",
      "Run 72/180 - Acc: 0.2258\n",
      "Run 73/180 - Acc: 0.3065\n",
      "Run 74/180 - Acc: 0.3443\n",
      "Run 75/180 - Acc: 0.2459\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.2987\n",
      "Run 76/180 - Acc: 0.5968\n",
      "Run 77/180 - Acc: 0.3226\n",
      "Run 78/180 - Acc: 0.2742\n",
      "Run 79/180 - Acc: 0.4590\n",
      "Run 80/180 - Acc: 0.3770\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.4059\n",
      "Run 81/180 - Acc: 0.5323\n",
      "Run 82/180 - Acc: 0.5323\n",
      "Run 83/180 - Acc: 0.5161\n",
      "Run 84/180 - Acc: 0.4426\n",
      "Run 85/180 - Acc: 0.3443\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.4735\n",
      "Run 86/180 - Acc: 0.2419\n",
      "Run 87/180 - Acc: 0.3226\n",
      "Run 88/180 - Acc: 0.2258\n",
      "Run 89/180 - Acc: 0.3770\n",
      "Run 90/180 - Acc: 0.4590\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'SGD', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3253\n",
      "Run 91/180 - Acc: 0.7742\n",
      "Run 92/180 - Acc: 0.8871\n",
      "Run 93/180 - Acc: 0.7258\n",
      "Run 94/180 - Acc: 0.8197\n",
      "Run 95/180 - Acc: 0.8361\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.8086\n",
      "üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: 0.8086\n",
      "Run 96/180 - Acc: 0.8065\n",
      "Run 97/180 - Acc: 0.9194\n",
      "Run 98/180 - Acc: 0.8387\n",
      "Run 99/180 - Acc: 0.8033\n",
      "Run 100/180 - Acc: 0.8033\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.8342\n",
      "üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: 0.8342\n",
      "Run 101/180 - Acc: 0.7097\n",
      "Run 102/180 - Acc: 0.8871\n",
      "Run 103/180 - Acc: 0.5484\n",
      "Run 104/180 - Acc: 0.8197\n",
      "Run 105/180 - Acc: 0.6721\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.7274\n",
      "Run 106/180 - Acc: 0.8710\n",
      "Run 107/180 - Acc: 0.8871\n",
      "Run 108/180 - Acc: 0.8065\n",
      "Run 109/180 - Acc: 0.8033\n",
      "Run 110/180 - Acc: 0.8033\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.8342\n",
      "Run 111/180 - Acc: 0.7419\n",
      "Run 112/180 - Acc: 0.9194\n",
      "Run 113/180 - Acc: 0.7903\n",
      "Run 114/180 - Acc: 0.8197\n",
      "Run 115/180 - Acc: 0.6885\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.7920\n",
      "Run 116/180 - Acc: 0.7419\n",
      "Run 117/180 - Acc: 0.8548\n",
      "Run 118/180 - Acc: 0.7419\n",
      "Run 119/180 - Acc: 0.7869\n",
      "Run 120/180 - Acc: 0.7213\n",
      "Params: {'learning_rate': 0.001, 'optimizer_name': 'Adam', 'batch_size': 32, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.7694\n",
      "Run 121/180 - Acc: 0.2742\n",
      "Run 122/180 - Acc: 0.4355\n",
      "Run 123/180 - Acc: 0.4355\n",
      "Run 124/180 - Acc: 0.4262\n",
      "Run 125/180 - Acc: 0.3607\n",
      "Params: {'learning_rate': 0.0005, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.0} | Avg Acc: 0.3864\n",
      "Run 126/180 - Acc: 0.3710\n",
      "Run 127/180 - Acc: 0.2742\n",
      "Run 128/180 - Acc: 0.4677\n",
      "Run 129/180 - Acc: 0.2295\n",
      "Run 130/180 - Acc: 0.2951\n",
      "Params: {'learning_rate': 0.0005, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.2} | Avg Acc: 0.3275\n",
      "Run 131/180 - Acc: 0.4194\n",
      "Run 132/180 - Acc: 0.2419\n",
      "Run 133/180 - Acc: 0.3548\n",
      "Run 134/180 - Acc: 0.2459\n",
      "Run 135/180 - Acc: 0.4098\n",
      "Params: {'learning_rate': 0.0005, 'optimizer_name': 'SGD', 'batch_size': 16, 'activation': 'relu', 'dropout_rate': 0.5} | Avg Acc: 0.3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 23:31:55.303188: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 73.78MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-14 23:31:55.303228: W tensorflow/core/framework/op_kernel.cc:1855] OP_REQUIRES failed at conv_ops_fused_impl.h:660 : UNKNOWN: CUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n",
      "2025-12-14 23:31:55.303244: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: CUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n",
      "\t [[{{function_node __inference_one_step_on_data_454922}}{{node sequential_1/conv2d_2_1/Relu}}]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/conv2d_2_1/Relu defined at (most recent call last):\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_354978/3229293629.py\", line 105, in <module>\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 134, in one_step_on_data\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 59, in train_step\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/function.py\", line 206, in _run_through_graph\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/functional.py\", line 644, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py\", line 263, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations/activations.py\", line 47, in relu\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations/activations.py\", line 101, in static_call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py\", line 15, in relu\n\nCUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n\t [[{{node sequential_1/conv2d_2_1/Relu}}]] [Op:__inference_multi_step_on_iterator_454963]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 105\u001b[0m\n\u001b[1;32m     98\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     99\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m),    \u001b[38;5;66;03m#ÂÖà‰∏ç‰øùÁïôÊùÉÈáç\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# ÁΩëÊ†ºÊêúÁ¥¢Êó∂ ReduceLROnPlateau ÂèØËÉΩÊãñÊÖ¢ÈÄüÂ∫¶ÔºåÁÆÄÂçïËµ∑ËßÅÂèØ‰ª•ÂÖàÊ≥®ÈáäÊéâÔºåÊàñËÄÖ‰øùÁïô\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# tf.keras.callbacks.ReduceLROnPlateau(patience=2) \u001b[39;00m\n\u001b[1;32m    102\u001b[0m ]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# ËÆ≠ÁªÉ (Êó†ÈúÄ GeneratorÔºåÁõ¥Êé•‰º†Êï∞ÁªÑ)\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ÈùôÈªòÊ®°ÂºèÔºåÂè™ÊâìÂç∞ÁªìÊûú\u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# ËØÑ‰º∞\u001b[39;00m\n\u001b[1;32m    115\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/conv2d_2_1/Relu defined at (most recent call last):\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_354978/3229293629.py\", line 105, in <module>\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 134, in one_step_on_data\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 59, in train_step\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/function.py\", line 206, in _run_through_graph\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/models/functional.py\", line 644, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py\", line 263, in call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations/activations.py\", line 47, in relu\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations/activations.py\", line 101, in static_call\n\n  File \"/home/zyh/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py\", line 15, in relu\n\nCUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n\t [[{{node sequential_1/conv2d_2_1/Relu}}]] [Op:__inference_multi_step_on_iterator_454963]"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model(activation=activation, dropout_rate=dropout_rate, l2_rate=l2_rate)\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], jit_compile=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ÂÖ®ÂÜÖÂ≠òÊï∞ÊçÆÂä†ËΩΩÂáΩÊï∞ (ÈÄüÂ∫¶‰ºòÂåñÁöÑÂÖ≥ÈîÆ)\n",
    "def load_data_to_memory(data_dir, img_size):\n",
    "    print(f\"Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆÂà∞ÂÜÖÂ≠ò: {data_dir} ...\")\n",
    "    X = []\n",
    "    y = []\n",
    "    # Á°Æ‰øùÁ±ªÂêçÈ°∫Â∫è‰∏ÄËá¥: ['apple', 'banana', 'mixed', 'orange']\n",
    "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    print(f\"Ê£ÄÊµãÂà∞ÁöÑÁ±ªÂà´: {classes}\")\n",
    "    \n",
    "    for label_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        files = os.listdir(class_dir)\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                fpath = os.path.join(class_dir, fname)\n",
    "                # Âä†ËΩΩÂπ∂Ë∞ÉÊï¥Â§ßÂ∞è\n",
    "                img = load_img(fpath, target_size=(img_size, img_size))\n",
    "                # ËΩ¨‰∏∫Êï∞ÁªÑÂπ∂ÂΩí‰∏ÄÂåñ (0-1)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                \n",
    "                X.append(img_array)\n",
    "                y.append(label_idx)\n",
    "                \n",
    "    X = np.array(X)\n",
    "    # Â∞ÜÊ†áÁ≠æËΩ¨‰∏∫ One-hot ÁºñÁ†Å (e.g., [0, 1, 0, 0])\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=len(classes))\n",
    "    print(f\"Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê! X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# ‰∏ÄÊ¨°ÊÄßËØªÂèñÊâÄÊúâÊï∞ÊçÆÂà∞ RAM (320Âº†ÂõæÁ∫¶Âç†Áî® 150MB~200MB ÂÜÖÂ≠òÔºåÈùûÂ∏∏ÂÆâÂÖ®)\n",
    "X_all, y_all = load_data_to_memory(train_dir, IMG_SIZE)\n",
    "\n",
    "\n",
    "#  ÊûÅÈÄüÁâà Grid Search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001,0.0005],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32],\n",
    "    'activation': ['relu'],\n",
    "    'dropout_rate': [0.0,0.2,0.5],\n",
    "    #'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "\n",
    "# ‰ΩøÁî®5Êäò‰∫§ÂèâÈ™åËØÅ\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "# ÁîüÊàêÊâÄÊúâÁªÑÂêà\n",
    "combinations = list(itertools.product(*param_grid.values()))\n",
    "total_runs = len(combinations) * 5\n",
    "current_run = 0\n",
    "\n",
    "print(f\"ÂºÄÂßã Grid SearchÔºåÊÄªÂÖ±Ë¶ÅËÆ≠ÁªÉ {len(combinations)} ‰∏™Ê®°ÂûãÈÖçÁΩÆ...\")\n",
    "\n",
    "for combo in combinations:\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    \n",
    "    # ‰ªé params ‰∏≠ÂàÜÁ¶ªÂá∫ batch_sizeÔºåÂõ†‰∏∫ÂÆÉ‰∏ç‰º†Áªô build_model\n",
    "    build_args = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "    current_batch_size = params['batch_size']\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    # K-Fold Âæ™ÁéØ\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "        # Ê∏ÖÁêÜÊóßÊ®°ÂûãÔºåÈáäÊîæÊòæÂ≠ò \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # ÂàíÂàÜÊï∞ÊçÆ (Áõ¥Êé•ÂÜÖÂ≠òÂàáÁâáÔºåÈÄüÂ∫¶ÊûÅÂø´)\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "        \n",
    "        # ÊûÑÂª∫Ê®°Âûã\n",
    "        model = build_model(**build_args)\n",
    "        \n",
    "        # ÂõûË∞ÉÂáΩÊï∞\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=False, monitor='val_loss'),    #ÂÖà‰∏ç‰øùÁïôÊùÉÈáç\n",
    "            # ÁΩëÊ†ºÊêúÁ¥¢Êó∂ ReduceLROnPlateau ÂèØËÉΩÊãñÊÖ¢ÈÄüÂ∫¶ÔºåÁÆÄÂçïËµ∑ËßÅÂèØ‰ª•ÂÖàÊ≥®ÈáäÊéâÔºåÊàñËÄÖ‰øùÁïô\n",
    "            # tf.keras.callbacks.ReduceLROnPlateau(patience=2) \n",
    "        ]\n",
    "        \n",
    "        # ËÆ≠ÁªÉ (Êó†ÈúÄ GeneratorÔºåÁõ¥Êé•‰º†Êï∞ÁªÑ)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=current_batch_size,\n",
    "            epochs=15, \n",
    "            callbacks=callbacks,\n",
    "            verbose=0  # ÈùôÈªòÊ®°ÂºèÔºåÂè™ÊâìÂç∞ÁªìÊûú\n",
    "        )\n",
    "        \n",
    "        # ËØÑ‰º∞\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        acc_list.append(val_acc)\n",
    "        \n",
    "        current_run += 1\n",
    "        # ÊâìÂç∞ËøõÂ∫¶Êù°\n",
    "        print(f\"Run {current_run}/{total_runs} - Acc: {val_acc:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    print(f\"Params: {params} | Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        best_params = params\n",
    "        print(f\"üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"ÊêúÁ¥¢ÁªìÊùü\")\n",
    "print(f\"ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc}\")\n",
    "print(f\"ÊúÄ‰Ω≥ÂèÇÊï∞: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above:\n",
    "\n",
    "We defined build_model to accept the hyperparams. If activation='leaky_relu', inside create_cnn_model we would handle that by setting layers with no activation and adding LeakyReLU layers. (This implementation detail can be handled with an if inside create_cnn_model.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ideally, each fold‚Äôs training could stop early if the model‚Äôs performance on that fold‚Äôs validation subset stops improving. The scikit-learn wrapper does not directly use the K-fold partition as a Keras validation in each fit call. A workaround is to use validation_split within each fold‚Äôs training or to manually perform the cross-validation loop. In our case, we set a relatively small number of epochs (10) for each training, assuming this is sufficient to evaluate performance without severe overfitting. If we wanted to incorporate early stopping in GridSearchCV, we could pass a callback through the fit_params. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# grid_search.fit(X_train, y_train, **{'callbacks': [early_stop], 'validation_split': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would use 10% of each fold‚Äôs training data as a validation for early stopping. However,  this means we‚Äôre not fully using that 10% for training in each fold (since it‚Äôs used as a temp validation), and the actual cross-val fold (held-out by GridSearch) isn‚Äôt directly used for early stopping. Due to these complexities, one might simply keep epochs low or perform manual K-fold training to properly utilize each fold‚Äôs validation. Given our epoch count is modest (10) and we have early stopping for the final training phase, we can omit early stopping during the grid search phase to simplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with Best Hyperparameters and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build final model with best hyperparams\n",
    "# best_model = create_cnn_model(activation=best_params['activation'], \n",
    "#                               dropout_rate=best_params['dropout_rate'], \n",
    "#                               l2_rate=best_params['l2_rate'])\n",
    "# if best_params['optimizer_name'] == 'Adam':\n",
    "#     final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "# else:\n",
    "#     final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_params['learning_rate'])\n",
    "# best_model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Setup callbacks for early stopping (and optional learning rate reduction, checkpoints as in original code)\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "#     ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model with early stopping\n",
    "# history = best_model.fit(train_generator, epochs=50,  # start with an upper bound, early stopping will likely stop earlier\n",
    "#                          validation_data=val_generator, \n",
    "#                          callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Visualization of Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_acc = best_model.evaluate(val_generator)\n",
    "# print(f\"Validation Accuracy: {val_acc:.2%}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract history data\n",
    "# epochs = range(1, len(history.history['loss'])+1)\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# train_acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# # Plot Loss Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "# plt.title('Training vs Validation Loss')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "# # Plot Accuracy Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "# plt.title('Training vs Validation Accuracy')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
