{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\" # ÂÖ≥Èó≠ÈÉ®ÂàÜ XLA Ë°å‰∏∫\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\" # Âº∫Âà∂Á°ÆÂÆöÊÄßÁÆóÊ≥ïÔºåÂáèÂ∞ëÊêúÁ¥¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing\n",
    "load and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"/home/zyh/Fruit-Classifier/data/train_augment\"    # directory with subfolders for each class (train set)\n",
    "\n",
    "\n",
    "IMG_SIZE = 224   # image size (can be adjusted as needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #Convolution blocks\n",
    "    model.add(layers.Conv2D(32, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2)) \n",
    "\n",
    "    model.add(layers.Conv2D(64, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2)) \n",
    "\n",
    "    model.add(layers.Conv2D(128, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(256, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Conv2D(512, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Conv2D(1024, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Conv2D(2048, kernel_size = (3,3), \n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(4,activation='softmax'))\n",
    "\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Wrap the model creation in KerasClassifier for use in GridSearchCV\n",
    "def build_model(learning_rate=0.01, optimizer_name='Adam', activation='relu', \n",
    "                dropout_rate=0.2, l2_rate=0.0):\n",
    "    # Build the CNN model with given hyperparams\n",
    "    model = create_cnn_model()\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:  # 'SGD'\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], jit_compile=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ÂÖ®ÂÜÖÂ≠òÊï∞ÊçÆÂä†ËΩΩÂáΩÊï∞ (ÈÄüÂ∫¶‰ºòÂåñÁöÑÂÖ≥ÈîÆ)\n",
    "def load_data_to_memory(data_dir, img_size):\n",
    "    print(f\"Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆÂà∞ÂÜÖÂ≠ò: {data_dir} ...\")\n",
    "    X = []\n",
    "    y = []\n",
    "    # Á°Æ‰øùÁ±ªÂêçÈ°∫Â∫è‰∏ÄËá¥: ['apple', 'banana', 'mixed', 'orange']\n",
    "    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    print(f\"Ê£ÄÊµãÂà∞ÁöÑÁ±ªÂà´: {classes}\")\n",
    "    \n",
    "    for label_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        files = os.listdir(class_dir)\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                fpath = os.path.join(class_dir, fname)\n",
    "                # Âä†ËΩΩÂπ∂Ë∞ÉÊï¥Â§ßÂ∞è\n",
    "                img = load_img(fpath, target_size=(img_size, img_size))\n",
    "                # ËΩ¨‰∏∫Êï∞ÁªÑÂπ∂ÂΩí‰∏ÄÂåñ (0-1)\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "                \n",
    "                X.append(img_array)\n",
    "                y.append(label_idx)\n",
    "                \n",
    "    X = np.array(X)\n",
    "    # Â∞ÜÊ†áÁ≠æËΩ¨‰∏∫ One-hot ÁºñÁ†Å (e.g., [0, 1, 0, 0])\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=len(classes))\n",
    "    print(f\"Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê! X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# ‰∏ÄÊ¨°ÊÄßËØªÂèñÊâÄÊúâÊï∞ÊçÆÂà∞ RAM (320Âº†ÂõæÁ∫¶Âç†Áî® 150MB~200MB ÂÜÖÂ≠òÔºåÈùûÂ∏∏ÂÆâÂÖ®)\n",
    "X_all, y_all = load_data_to_memory(train_dir, IMG_SIZE)\n",
    "\n",
    "\n",
    "#  ÊûÅÈÄüÁâà Grid Search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001,0.0005],\n",
    "    'optimizer_name': ['SGD', 'Adam'],\n",
    "    'batch_size': [16, 32],\n",
    "    #'activation': ['relu'],\n",
    "    'dropout_rate': [0.0,0.2],\n",
    "    #'l2_rate': [0.0, 0.001]\n",
    "}\n",
    "\n",
    "# ‰ΩøÁî®5Êäò‰∫§ÂèâÈ™åËØÅ\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "# ÁîüÊàêÊâÄÊúâÁªÑÂêà\n",
    "combinations = list(itertools.product(*param_grid.values()))\n",
    "total_runs = len(combinations) * 5\n",
    "current_run = 0\n",
    "\n",
    "print(f\"ÂºÄÂßã Grid SearchÔºåÊÄªÂÖ±Ë¶ÅËÆ≠ÁªÉ {len(combinations)} ‰∏™Ê®°ÂûãÈÖçÁΩÆ...\")\n",
    "\n",
    "for combo in combinations:\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    \n",
    "    # ‰ªé params ‰∏≠ÂàÜÁ¶ªÂá∫ batch_sizeÔºåÂõ†‰∏∫ÂÆÉ‰∏ç‰º†Áªô build_model\n",
    "    build_args = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "    current_batch_size = params['batch_size']\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    # K-Fold Âæ™ÁéØ\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "        # Ê∏ÖÁêÜÊóßÊ®°ÂûãÔºåÈáäÊîæÊòæÂ≠ò \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # ÂàíÂàÜÊï∞ÊçÆ (Áõ¥Êé•ÂÜÖÂ≠òÂàáÁâáÔºåÈÄüÂ∫¶ÊûÅÂø´)\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "        \n",
    "        # ÊûÑÂª∫Ê®°Âûã\n",
    "        model = build_model(**build_args)\n",
    "        \n",
    "        # ÂõûË∞ÉÂáΩÊï∞\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=False, monitor='val_loss'),    #ÂÖà‰∏ç‰øùÁïôÊùÉÈáç\n",
    "            # ÁΩëÊ†ºÊêúÁ¥¢Êó∂ ReduceLROnPlateau ÂèØËÉΩÊãñÊÖ¢ÈÄüÂ∫¶ÔºåÁÆÄÂçïËµ∑ËßÅÂèØ‰ª•ÂÖàÊ≥®ÈáäÊéâÔºåÊàñËÄÖ‰øùÁïô\n",
    "            # tf.keras.callbacks.ReduceLROnPlateau(patience=2) \n",
    "        ]\n",
    "        \n",
    "        # ËÆ≠ÁªÉ (Êó†ÈúÄ GeneratorÔºåÁõ¥Êé•‰º†Êï∞ÁªÑ)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=current_batch_size,\n",
    "            epochs=15, \n",
    "            callbacks=callbacks,\n",
    "            verbose=0  # ÈùôÈªòÊ®°ÂºèÔºåÂè™ÊâìÂç∞ÁªìÊûú\n",
    "        )\n",
    "        \n",
    "        # ËØÑ‰º∞\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        acc_list.append(val_acc)\n",
    "        \n",
    "        current_run += 1\n",
    "        # ÊâìÂç∞ËøõÂ∫¶Êù°\n",
    "        print(f\"Run {current_run}/{total_runs} - Acc: {val_acc:.4f}\")\n",
    "\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    print(f\"Params: {params} | Avg Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        best_params = params\n",
    "        print(f\"üî•üî•üî• ÂèëÁé∞Êñ∞ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"ÊêúÁ¥¢ÁªìÊùü\")\n",
    "print(f\"ÊúÄ‰Ω≥Á≤æÂ∫¶: {best_acc}\")\n",
    "print(f\"ÊúÄ‰Ω≥ÂèÇÊï∞: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above:\n",
    "\n",
    "We defined build_model to accept the hyperparams. If activation='leaky_relu', inside create_cnn_model we would handle that by setting layers with no activation and adding LeakyReLU layers. (This implementation detail can be handled with an if inside create_cnn_model.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# grid_search.fit(X_train, y_train, **{'callbacks': [early_stop], 'validation_split': 0.1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with Best Hyperparameters and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build final model with best hyperparams\n",
    "# best_model = create_cnn_model(activation=best_params['activation'], \n",
    "#                               dropout_rate=best_params['dropout_rate'], \n",
    "#                               l2_rate=best_params['l2_rate'])\n",
    "# if best_params['optimizer_name'] == 'Adam':\n",
    "#     final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "# else:\n",
    "#     final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_params['learning_rate'])\n",
    "# best_model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Setup callbacks for early stopping (and optional learning rate reduction, checkpoints as in original code)\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "#     ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model with early stopping\n",
    "# history = best_model.fit(train_generator, epochs=50,  # start with an upper bound, early stopping will likely stop earlier\n",
    "#                          validation_data=val_generator, \n",
    "#                          callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Visualization of Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_acc = best_model.evaluate(val_generator)\n",
    "# print(f\"Validation Accuracy: {val_acc:.2%}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract history data\n",
    "# epochs = range(1, len(history.history['loss'])+1)\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# train_acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# # Plot Loss Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "# plt.title('Training vs Validation Loss')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "# # Plot Accuracy Curves\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "# plt.title('Training vs Validation Accuracy')\n",
    "# plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
